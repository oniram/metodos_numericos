{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capítulo 4 - Derivação e Integração Numérica"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introdução"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No capítulo anterior trabalhamos com técnicas de interpolação numérica, que são bastante úteis quando precisamos aproximar o valor de uma função em pontos de um determinado intervalo. No entanto, quando existe a necessidade de obter informações mais complexas, como áreas de superfícies e volumes, a aplicação de técnicas simples de interpolação não é o suficiente.\n",
    "\n",
    "Neste capítulo desenvolveremos o conceito de derivação e integração numérica. Para isso, utilizaremos os polinômios obtidos através de técnicas de interpolação, uma vez que o processo de derivar e integrar polinômios é bastante simples, e efetivo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Derivação numérica"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pela definição de derivada de uma função $f(x)$, denominada $f'(x)$, temos:\n",
    "\n",
    "$$f'(x_0) = \\lim_{h→0} \\frac{f(x_0+h)-f(x_0)}{h}$$\n",
    "\n",
    "Podemos aproximar esse resultado utilizando o **polinômio de Lagrange**. Para isso, precisamos fazer as seguintes considerações:\n",
    "* $x_0 \\in (a,b)$\n",
    "* $x_1 \\in [a,b]$\n",
    "* $x_1 = x_0 + h$ com $h \\ne 0$ \n",
    "* $f'(x_0) \\in C^2 [a,b]$\n",
    "\n",
    "Dessa forma:\n",
    "\n",
    "$$f(x) = P_{0,1}(x) + \\frac{(x-x_0)(x-x_1)}{2!}f''(\\xi(x))$$\n",
    "\n",
    "$$f(x) = f(x_0)\\frac{x-x_1}{x_0-x_1}+f(x_1)\\frac{x-x_0}{x_1-x_0} +f''(\\xi(x))\\frac{(x-x_0)(x-x_1)}{2}$$\n",
    "\n",
    "$$f(x) = f(x_0)\\frac{x-x_0-h}{-h}+f(x_0+h)\\frac{x-x_0}{h} +f''(\\xi(x))\\frac{(x-x_0)(x-x_0-h)}{2}$$\n",
    "\n",
    "Derivando a equação acima, temos:\n",
    "\n",
    "$$f'(x) = \\frac{d\\frac{f(x_0)x -f(x_0)x_0 -f(x_0)h}{-h}}{dx}+\\frac{d\\frac{f(x_0+h)x -f(x_0+h)x_0}{h}}{dx} +\\frac{d[f''(\\xi(x))\\frac{(x-x_0)(x-x_0-h)}{2}]}{dx}$$\n",
    "\n",
    "$$f'(x) = \\frac{f(x_0+h) - f(x_0)}{h} + \\frac{2(x-x_0)-h}{2}f''(\\xi(x))+\\frac{(x-x_0)(x-x_0-h)}{2}\\,.\\,\\frac{d[f''(\\xi(x))]}{dx}$$\n",
    "\n",
    "O que pode ser resumido como:\n",
    "\n",
    "$$f'(x) \\approx \\frac{f(x_0+h)-f(x_0)}{h}$$\n",
    "\n",
    "O problema com esta aproximação é devivo à falta de informações a respeito do termo $\\frac{d[f''(\\xi(x))]}{dx}$, o que impossibilita a estimativa do erro de truncamento. Entretanto, sabemos que quando $x=x_0$ o coeficiente do termo problemático é 0, e a fórmula pode ser simplificada para:\n",
    "\n",
    "$$f'(x) = \\frac{f(x_0+h) - f(x_0)}{h} - \\frac{h}{2}f''(\\xi(x))$$\n",
    "\n",
    "Dessa forma, quando tratamos de valores pequenos de h, o limitante do erro será dado por $M \\frac{|h|}{2}$, em que $M$ é um limitande da função $f''(x)$ para $x \\in [a,b]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Isso é conhecido como **fórmula de diferenças progressivas** se $h>0$.\n",
    "\n",
    "#### Erro de truncamento por diferenças progressivas##\n",
    "Seja $D_{+,h}f(x_{0})$ a aproximação da derivada de $f$ em $x_{0}$ por difereças progressivas:\n",
    "\n",
    "$$D_{+,h}f(x_{0}) - f´(x_{0})= \\frac {f(x_{0}+h) - f(x_{0})}{h} -f´(x_{0})$$\n",
    "\n",
    "$$ \\frac {f(x_{0}) +hf´(x_{0}) + h^{2}/2f´´(x_{0})+O(h^3) - f(x_{0})}{h} - f´(x_{0})$$\n",
    "\n",
    "$$h/2f´´(x_{0})+O(h^{2}) = O(h)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extrapolação de Richardson"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A extrapolação de Richardson é um __método para aceleração de convergência__, que tem como principal objetivo __aumentar a precisão de outros métodos__ através da aproximação do valor do erro inato de tais métodos. Por este motivo, este procedimento só pode ser utilizado quando for possível obter a fórmula do erro de uma aproximação em função de um parâmetro (geralmente o passo \"h\").\n",
    "\n",
    "Para entender como se obtém um aumento da precisão dos resultados utilizando a extrapolação de Richardson, precisamos relembrar o conceito de __taxa de convergência__, e as convenções anteriormente estabelecidas para a mesma.\n",
    "\n",
    "> A __taxa de convergência__ $O(\\beta_n)$ indica o quão rápido um termo $\\alpha_n$ converge para seu valor real $\\alpha$ de tal forma que $|\\alpha_n-\\alpha| \\leq K|\\beta_n|$, para valores altos de n. Em geral, utilizamos $\\beta_n=\\frac{1}{n^p}$, o que resulta na fórmula $\\alpha_n = \\alpha + O(1/n^p)$.\n",
    "\n",
    "Com a definição acima, podemos perceber que, dada um método qualquer para a aproximação de um valor M da derivada de uma função desconhecida em um ponto $x_0$, podemos escrever uma equação do método que seja do tipo:\n",
    "\n",
    "$$M = N(h) +\\sum_{j=1}^{m-1}K_jh^j + O(h^m)$$\n",
    "\n",
    "Devemos observar que no início deste capítulo assumimos que para as aproximações dos valores das derivadas numéricas em um ponto, utilizaríamos valores pequenos (menores que 1) de h, o que significa que nossa fórmula para a taxa de convergência $O(h^m)$ está de acordo com o padrão desejável estabelecido na definição acima, do tipo $O(1/n^p)$. Além disso, o termo $O(h^m)$ se refere ao erro de truncamento, e pode ser interpretado como \"uma soma de termos de ordem m ou superior\".\n",
    "\n",
    "Observe ainda que, sendo as constantes $K_j$ desconhecidas, e sem grande variação de magnitude, podemos assumir que:\n",
    "\n",
    "$$M-N(0.1) \\approx 0.1K_1, \\quad M-N(0.01) \\approx 0.01 K_1$$\n",
    "\n",
    "e dessa forma, o erro de truncamento seria $O(h)$. Observe que podemos reduzir o valor de $O(h)$, se trabalharmos para obter uma fórmula com $O(h^2)$, que resultaria em:\n",
    "\n",
    "$$M-\\hat{N}(0.1) \\approx 0.001\\hat{K_2}, \\quad M-\\hat{N}(0.01) \\approx 0.0001 \\hat{K_2}$$\n",
    "\n",
    "É dessa maneira que o Método de Richardson funciona, buscando __combinar aproximações dos erros de maneira a obter fórmulas de erro de ordem superior__, o que se traduz em __resultados mais precisos__. Vejamos abaixo como o método funciona.\n",
    "\n",
    "Partindo da equação: $M = N(h) + K_1h + K_2h^2 + K_3h^3+ ...$, cujo erro de truncamento será $O(h)$, podemos substituir $h$ por $h/2$, de maneira a obter:\n",
    "\n",
    "$$M = N(\\frac{h}{2}) + K_1\\frac{h}{2} + K_2\\frac{h^2}{4}+K_3\\frac{h^3}{8}+ ...$$\n",
    "\n",
    "Subtraindo a equação para h da equação para h/2 duplicada, temos:\n",
    "\n",
    "$$M = [2N(\\frac{h}{2}) - N(h)] + K_2(\\frac{h^2}{2} - h^2) + K_3(\\frac{h^3}{4} -h^3) + ...$$\n",
    "\n",
    "Note que na equação acima, o termo acompanhado de $K_1$ foi eliminado pela subtração. Fazendo $N_1(h) \\equiv N(h)$, temos que, o termo entre colchetes na equação anterior é o nosso $N_2(h)$, ou seja:\n",
    "\n",
    "$$N_2(h) = [2N_1(\\frac{h}{2}) - N_1(h)] = N_1(\\frac{h}{2}) + [N_1(\\frac{h}{2}) -N_1(h)]$$\n",
    "\n",
    "Dessa forma, o novo erro de truncamento de nossa equação $M = N_2(h) +  K_2(\\frac{h^2}{2} - h^2) + K_3(\\frac{h^3}{4} -h^3) + ...$ é $O(h^2)$, ou seja. A nova precisão é superior à anterior. \n",
    "\n",
    "Os valores de $N_j$ nada mais são que __versões mais precisas do método utilizado como base para aproximar M__, uma vez que tais aproximações possuem um erro de truncamento $O(h^j)$. Generalizando o procedimento acima, obtemos a fórmula de $N_j$ que é dada por:\n",
    "\n",
    "$$N_j(h) = N_{j-1}(\\frac{h}{2}) + \\frac{N_{j-1}(\\frac{h}{2}) - N_{j-1}(h)}{2^{j-1}-1}$$\n",
    "\n",
    "Que é uma __fórmula de recorrência__, uma vez que utiliza os valores das iterações anteriores para obter valores da nova iteração.\n",
    "\n",
    "O __limite do erro da extrapolação de Richardson__ pode ser estimado calculando-se o valor de K. Para isso, consideramos a equação:\n",
    "\n",
    "$$M = N_j(h) +K_{j}h^j + O(h^{j+1})$$\n",
    "\n",
    "que nada mais é que uma forma alternativa da equação $M = N(h) +\\sum_{j=1}^{m-1}K_jh^j + O(h^m)$, que utiliza o valor dos $N_j$ para deixar somente um coeficiente $K_j$ por vez. Sabemos que, como $h<1$ e o termo $O(h^{j+1})$ é uma função de $h^{j+1}$, este terá um valor menor que $K_jh^j$, e pode ser desconsiderado sem grandes prejuizos (especialmente para valores altos de j). Dessa forma, subtraindo a equação acima com h/2 da mesma equação para h, temos:\n",
    "\n",
    "$$0 = N_j(h) - N_j(\\frac{h}{2}) + K_jh^j-K_j(\\frac{h}{2})^j + O^*(h^{j+1})$$\n",
    "\n",
    "$$0 = N_j(h) - N_j(\\frac{h}{2}) + K_jh^j(1-\\frac{1}{2})^j + O^*(h^{j+1})$$\n",
    "\n",
    "$$ K_j = \\frac{N_j(\\frac{h}{2}) - N_j(h)}{h^j(1-\\frac{1}{2})^j} + O^*(h^{j+1})$$\n",
    "\n",
    "Assim, temos que o erro $E(h)$ pode ser estimado por:\n",
    "\n",
    "$$E(h) = 2^j\\frac{N_j(\\frac{h}{2}) - N_j(h)}{2^j-1} + O(h^{j+1})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Referências:\n",
    "\n",
    "* BURDEN, R.L.;FAIRES,D.J.;BURDEN, A.M. **Numerical Analysis**. 8 ed. Boston, MA: Cengage Learning, 2014, cap. 4, p.167-247. ISBN 978-1-305-25366-7\n",
    "\n",
    "* http://www.mat.ufrgs.br/~fabio/der_int.pdf\n",
    "\n",
    "* http://www.math.ubc.ca/~feldman/m256/richard.pdf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
