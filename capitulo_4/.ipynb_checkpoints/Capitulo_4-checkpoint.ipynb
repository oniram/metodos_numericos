{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capítulo 4 - Derivação e Integração Numérica"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introdução"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No capítulo anterior trabalhamos com técnicas de interpolação numérica, que são bastante úteis quando precisamos aproximar o valor de uma função em pontos de um determinado intervalo. No entanto, quando existe a necessidade de obter informações mais complexas, como áreas de superfícies e volumes, a aplicação de técnicas simples de interpolação não é o suficiente.\n",
    "\n",
    "Neste capítulo desenvolveremos o conceito de derivação e integração numérica. Para isso, utilizaremos os polinômios obtidos através de técnicas de interpolação, uma vez que o processo de derivar e integrar polinômios é bastante simples, e efetivo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Derivação numérica"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pela definição de derivada de uma função $f(x)$, denominada $f'(x)$, temos:\n",
    "\n",
    "$$f'(x_0) = \\lim_{h→0} \\frac{f(x_0+h)-f(x_0)}{h}$$\n",
    "\n",
    "Podemos aproximar esse resultado utilizando o **polinômio de Lagrange**. Para isso, precisamos fazer as seguintes considerações:\n",
    "* $x_0 \\in (a,b)$\n",
    "* $x_1 \\in [a,b]$\n",
    "* $x_1 = x_0 + h$ com $h \\ne 0$ \n",
    "* $f'(x_0) \\in C^2 [a,b]$\n",
    "\n",
    "Dessa forma:\n",
    "\n",
    "$$f(x) = P_{0,1}(x) + \\frac{(x-x_0)(x-x_1)}{2!}f''(\\xi(x))$$\n",
    "\n",
    "$$f(x) = f(x_0)\\frac{x-x_1}{x_0-x_1}+f(x_1)\\frac{x-x_0}{x_1-x_0} +f''(\\xi(x))\\frac{(x-x_0)(x-x_1)}{2}$$\n",
    "\n",
    "$$f(x) = f(x_0)\\frac{x-x_0-h}{-h}+f(x_0+h)\\frac{x-x_0}{h} +f''(\\xi(x))\\frac{(x-x_0)(x-x_0-h)}{2}$$\n",
    "\n",
    "Derivando a equação acima, temos:\n",
    "\n",
    "$$f'(x) = \\frac{d\\frac{f(x_0)x -f(x_0)x_0 -f(x_0)h}{-h}}{dx}+\\frac{d\\frac{f(x_0+h)x -f(x_0+h)x_0}{h}}{dx} +\\frac{d[f''(\\xi(x))\\frac{(x-x_0)(x-x_0-h)}{2}]}{dx}$$\n",
    "\n",
    "$$f'(x) = \\frac{f(x_0+h) - f(x_0)}{h} + \\frac{2(x-x_0)-h}{2}f''(\\xi(x))+\\frac{(x-x_0)(x-x_0-h)}{2}\\,.\\,\\frac{d[f''(\\xi(x))]}{dx}$$\n",
    "\n",
    "O que pode ser resumido como:\n",
    "\n",
    "$$f'(x) \\approx \\frac{f(x_0+h)-f(x_0)}{h}$$\n",
    "\n",
    "O problema com esta aproximação é devivo à falta de informações a respeito do termo $\\frac{d[f''(\\xi(x))]}{dx}$, o que impossibilita a estimativa do erro de truncamento. Entretanto, sabemos que quando $x=x_0$ o coeficiente do termo problemático é 0, e a fórmula pode ser simplificada para:\n",
    "\n",
    "$$f'(x) = \\frac{f(x_0+h) - f(x_0)}{h} - \\frac{h}{2}f''(\\xi(x))$$\n",
    "\n",
    "Dessa forma, quando tratamos de valores pequenos de h, o limitante do erro será dado por $M \\frac{|h|}{2}$, em que $M$ é um limitande da função $f''(x)$ para $x \\in [a,b]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Isso é conhecido como **fórmula de diferenças progressivas** se $h>0$.\n",
    "\n",
    "#### Erro de truncamento por diferenças progressivas##\n",
    "Seja $D_{+,h}f(x_{0})$ a aproximação da derivada de $f$ em $x_{0}$ por difereças progressivas:\n",
    "\n",
    "$$D_{+,h}f(x_{0}) - f´(x_{0})= \\frac {f(x_{0}+h) - f(x_{0})}{h} -f´(x_{0})$$\n",
    "\n",
    "$$ \\frac {f(x_{0}) +hf´(x_{0}) + h^{2}/2f´´(x_{0})+O(h^3) - f(x_{0})}{h} - f´(x_{0})$$\n",
    "\n",
    "$$h/2f´´(x_{0})+O(h^{2}) = O(h)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilização de múltiplos pontos para estimar a derivada\n",
    "\n",
    "Para obter fórmulas gerais de aproximação de derivadas, suponha que ${x_{0},x_{1},x_{2},...,x_{n}}$ são $(n+1)$ números distintos em um certo intervalo $I$ e que $f \\in C^{(n+1)}(I)$. Sabemos que o polinômio interpolador de Lagrange é dado por:\n",
    "\n",
    "\n",
    "$$f(x)=\\sum_{k=0}^{n} f(x_{k})L_{k}(x) + \\frac{(x-x_{0})...(x-x_{n})}{(n+1)!}f^{(n+1)}(\\xi(x)) $$\n",
    "\n",
    "para algum $(\\xi(x))$ em $I$, onde $L_{k}(x)$ é o *k-ésimo* coeficiente do Polinômio de Lagrange para $f$ em dados $x_{0},x_{1},x_{2},...,x_{n}$.\n",
    "\n",
    "Quando derivamos a expressão anterior teremos:\n",
    "\n",
    "$${f}'(x)=\\sum_{k=0}^{n} f(x_{k}){L}'_{k}(x)+D_{x}\\left [ \\frac{(x-x_{0})...(x-x_{n})}{(n+1)!} \\right ]f^{(n+1)}(\\xi(x)) + \\frac{(x-x_{0})...(x-x_{n})}{(n+1)!}D_{x}\\left [ f^{(n+1)}(\\xi(x)) \\right ]$$\n",
    "\n",
    "E, novamente nos encontramos com o problema do erro de truncamento, a menos que $x$ pertença aos pontos dados no início do problema, ou seja $x \\in {x_{0},x_{1},x_{2},...,x_{n}}$. Neste caso teremos que os termos $D_{x}\\left [ f^{(n+1)}(\\xi(x)) \\right ] = 0$, e a formula se resumirá:\n",
    "\n",
    "$${f}'(x_{j})=\\sum_{k=0}^{n} f(x_{k}){L}'_{k}(x_{j})+\\frac{f^{(n+1)}(\\xi(x_{j}))}{(n+1)!}$$<center>(1.0)</center>\n",
    "\n",
    "Denomina-se **formula de $(n+1)$ pontos** para obter uma aproximação de ${f}'(x_{j})$.\n",
    "\n",
    "### Vantagens e limitações.\n",
    "\n",
    "Em geral, quando utilizamos mais pontos na equação acima, produziremos maior precisão, porém o número de cálculos de funções e o crescimento de erros de arredondamento tornam este trabalho desencorajador. Portanto, as fórmulas mais comuns são as que envolvem 3 e 5 pontos de cálculo.\n",
    "\n",
    "### Fórmula do três pontos e Fórmula dos cinco pontos\n",
    "\n",
    "#### Fórmula do 3 pontos\n",
    "\n",
    "Primeiro deduziremos a fórmula de 3 pontos. Observe que, dado ${x_{0},x_{1}, x_{2}}$, temos que:\n",
    "\n",
    "$$L_{0}(x) = \\frac{(x-x_{1})(x-x_{2})}{(x_{0}-x_{1})(x_{0}-x_{2})}$$\n",
    "\n",
    "E, derivando:\n",
    "\n",
    "$${L}'_{0}(x) = \\frac{2x-x_{1}-x_{2}}{(x_{0}-x_{1})(x_{0}-x_{2})}$$\n",
    "\n",
    "Analogamente,\n",
    "\n",
    "$${L}'_{1}(x) = \\frac{2x-x_{0}-x_{2}}{(x_{1}-x_{0})(x_{1}-x_{2})}$$ e $${L}'_{2}(x) = \\frac{2x-x_{0}-x_{2}}{(x_{2}-x_{0})(x_{2}-x_{1})}$$\n",
    "\n",
    "Consequentemente, da **formula de  $(n+1)$ pontos**, decorre:\n",
    "\n",
    "$${f}'(x_{j})=f(x_{0})\\left [ \\frac{ 2x_{j}-x_{1}-x_{2}}{(x_{0}-x_{1})(x_{0}-x_{2})} \\right ]+\n",
    "f(x_{1})\\left [ \\frac{ 2x_{j}-x_{0}-x_{2}}{(x_{1}-x_{0})(x_{1}-x_{2})} \\right ] + \n",
    "f(x_{2})\\left [ \\frac{ 2x_{j}-x_{0}-x_{1}}{(x_{2}-x_{0})(x_{2}-x_{1})} \\right ] + \n",
    "\\frac{1}{6}f^{(3)}(\\xi_{j})\\prod_{\\begin{matrix}\n",
    "k=0\n",
    "\\\\k \\neq j\n",
    "\\end{matrix}}^{2}(x_{j}-x_{k})$$\n",
    "\n",
    "\n",
    "Para cada $j=0,1,2$ em que a notação $\\xi_{j}$ indica que esse ponto dependa de $x_{j}$.\n",
    "\n",
    "Observe que se os pontos forem igualemente espaçados, ou seja:\n",
    "\n",
    "<center>$x_{1} = x_{0} + h$  e  $x_{2}=x_{0}+2h$ para algum $h \\neq 0$</center>\n",
    "\n",
    "Se, $x_{j}=x_{0}, x_{1}=x_{0}+h$ e $x_{2}=x_{0}+2h$, temos:\n",
    "\n",
    "$${f}'(x_{0}) = -\\frac{3f(x_{0})+4f(x_{1})+f(x_{2})}{2h} + \\frac{h^{2}}{3}f^{(3)}(\\xi_{0})$$\n",
    "\n",
    "Da mesma forma ao fazermos $x_{j}=x_{2}$, temos:\n",
    "\n",
    "$${f}'(x_{1})= \\frac{f(x_{2})-f(x_{0})}{2h}+\\frac{h^2}{6}f^{(3)}(\\xi_{1})$$\n",
    "\n",
    "E, para $x_{j}=x_{2}$,\n",
    "\n",
    "$${f}'(x_{2})= \\frac{f(x_{0})-2f(x_{1})+3f(x_{2})}{2h} + \\frac{h^{2}}{6}f^{(3)}(\\xi_{2})$$\n",
    "\n",
    "Como $x_{1}=x_{0}+h$ e $x_{2}=x_{0}+2h$, essas formulas também podem ser expressas como:\n",
    "\n",
    "Equação 1:\n",
    "\n",
    "$${f}'(x_{0}) = -\\frac{3f(x_{0})+4f(x_{0}+h)+f(x_{0}+2h)}{2h} + \\frac{h^{2}}{3}f^{(3)}(\\xi_{0})$$\n",
    "\n",
    "Equação 2:\n",
    "$${f}'(x_{0}+h)= \\frac{f(x_{0}+2h)-f(x_{0})}{2h}+\\frac{h^2}{6}f^{(3)}(\\xi_{1})$$\n",
    "\n",
    "Equação 3:\n",
    "$${f}'(x_{0}+2h)= \\frac{f(x_{0})-4f(x_{0}+h)+3f(x_{0}+2h)}{2h} + \\frac{h^{2}}{6}f^{(3)}(\\xi_{2})$$\n",
    "\n",
    "Por conveniência substituimos a variavel $x_{0}$ por $x_{0}-h$ na **equação 2**, e $x_{0}$ por $x_{0}-2h$ na **equação 2**. Desta forma alteramos as formulas para aproximação de ${f}'(x_{0})$. Ficando com a seguinte caracteristica:\n",
    "\n",
    "Equação 1:\n",
    "\n",
    "$${f}'(x_{0}) = -\\frac{3f(x_{0})+4f(x_{0}+h)+f(x_{0}+2h)}{2h} + \\frac{h^{2}}{3}f^{(3)}(\\xi_{0})$$,\n",
    "\n",
    "Equação 2:\n",
    "$${f}'(x_{0})= \\frac{f(x_{0}+h)-f(x_{0}-h)}{2h}+\\frac{h^2}{6}f^{(3)}(\\xi_{1})$$,\n",
    "\n",
    "Equação 3:\n",
    "$${f}'(x_{0})= \\frac{f(x_{0}-2h)-4f(x_{0}-h)+3f(x_{0})}{2h} + \\frac{h^{2}}{6}f^{(3)}(\\xi_{2})$$\n",
    "\n",
    "Assim, as 3 formulas encontradas poderão ser utilizada para aproximação de $f'(x_{0})$:\n",
    "\n",
    "Observe que ao substituir $h$ por $-h$ na **equação 3**, temos que de fato só exista 2 fórmulas:\n",
    "\n",
    "**1º Em que $\\xi_{0}$ está entre $x_{0}$ e $x_{0}+2h$ (E 1.0)**\n",
    "\n",
    "> $${f}'(x_{0}) = -\\frac{3f(x_{0})+4f(x_{0}+h)+f(x_{0}+2h)}{2h} + \\frac{h^{2}}{3}f^{(3)}(\\xi_{0})$$,\n",
    "\n",
    "**2º Em que $\\xi_{1}$ está entre $(x_{0}-h)$ e $(x_{0}+h)$: (diferença centrada)(E 1.1)**\n",
    "\n",
    "> $${f}'(x_{0})= \\frac{f(x_{0}+h)-f(x_{0}-h)}{2h}+\\frac{h^2}{6}f^{(3)}(\\xi_{1})$$\n",
    "\n",
    "\n",
    "**Observações:**\n",
    "    \n",
    "   a) O erro na equação (1.1) é aproximadamente metade do erro na equação (1.0), pois (1.1) usa dados em ambos os lados de $x_{0}$;\n",
    "    \n",
    "   b) Também, na equação (1.1) precisamos apenas do cálculo em 2 pontos, enquanto que na equação (1.0) precisaremos dos cálculos em 3 pontos.\n",
    "    \n",
    "   c) Utiliza-se a equação (1.0) quando queremos valores próximos às extremidades de um intervalo.\n",
    "\n",
    "\n",
    "#### Fórmula dos 5 pontos\n",
    "\n",
    "\n",
    "Na seção anterior vimos que por meio de um polinômio integrador de Lagrange para $f$ deduzimos o método de três pontos. O metodo de 5 pontos (five-points), também pode ser obtidos de forma semelhante. Mas, utilizaremos a expansão de um polinômio de Taylor.\n",
    "\n",
    "\n",
    "Suponha que a função $f$ seja expandida até o quarto polinômio de Taylor em $x_{0}$. Então,\n",
    "\n",
    "\n",
    "$$f(x) = f(x_{0})+{f}'(x_{0})(x-x_{0})+\\frac{1}{2}{f}''(x_{0})(x-x_{0})^{2} + \\frac{1}{6}{f}'''(x_{0})(x-x_{0})^{3} + \\frac{1}{24}f_{(4)}(x_{0})(x-x_{0})^{4}+\\frac{1}{120}f^{(5)}(\\xi)(x-x_{0})^{5}$$\n",
    "\n",
    "Calculando em $f$, para $x_{0}-h$ e $x_{0}+h$, obtemos:\n",
    "\n",
    "$$f(x_{0}+h) = f(x_{0})+{f}'(x_{0})h+\\frac{1}{2}{f}''(x_{0})h^{2} + \\frac{1}{6}{f}'''(x_{0})h^{3} + \\frac{1}{24}f_{(4)}(x_{0})h^{4}+\\frac{1}{120}f^{(5)}(\\xi)h^{5}$$\n",
    "\n",
    "e,\n",
    "\n",
    "$$f(x_{0}-h) = f(x_{0})-{f}'(x_{0})h+\\frac{1}{2}{f}''(x_{0})h^{2} - \\frac{1}{6}{f}'''(x_{0})h^{3} + \\frac{1}{24}f_{(4)}(x_{0})h^{4}-\\frac{1}{120}f^{(5)}(\\xi)h^{5}$$\n",
    "\n",
    "Subtraindo $f(x_{0}+h) - f(x_{0}-h)$. Temos:\n",
    "\n",
    "$$f(x_{0}+h) - f(x_{0}-h)=2h{f}'(x_{0})+\\frac{h^{3}}{3}{f}'''(x_{0})+\\frac{h^{5}}{120}[f^{(5)(\\xi_{1}}+f^{(5)}(\\xi_{2})]$$\n",
    "\n",
    "sendo que $x_{0}-h<\\xi_{2}<x_{0}<\\xi_{1}<x_{0}+h$.\n",
    "\n",
    "Se $f^{(5)}$ for contínua em $[x_{0}-h,x_{0}+h]$, o Teorema do valor Intermediário nos permite dizer que existe em número $\\xi \\in (x_{0}-h, x_{0}+h)$, tal que:\n",
    "\n",
    "$$f^{(5)}(\\xi)=\\frac{1}{2}[f^{(5)(\\xi_{1}}+f^{(5)}(\\xi_{2})]$$\n",
    "\n",
    "Consequentemente, podemos isolar ${f}'(x_{0})$, e obtemos:\n",
    "\n",
    "Equação (2.0):\n",
    "$${f}'(x_{0})=\\frac{1}{2h}[f(x_{0}+h) - f(x_{0}-h)]-\\frac{h^{2}}{6}{f}'''(x_{0})-\\frac{h^{4}}{120}f^{(4)}(\\xi)$$\n",
    "\n",
    "Embora a aproximação na equação anterior seja a mesma fornecida na fórmula de três pontos, o ponto de cálculo desconhecido ocorre agora em ${f}'''$. A extrapolação tira vantagem disse ao substituir $h$ na equação anterior por $2h$ no qual $\\xi \\in (x_{0}-2h, x_{0}+2h)$. Disto decorre que:\n",
    "\n",
    "Equação (2.1)\n",
    "$${f}'(x_{0}) = \\frac{1}{4h}[f(x_{0}+2h) - f(x_{0}-2h)]-\\frac{4h^{2}}{6}{f}'''(x_{0})-\\frac{16h^{4}}{120}f^{(5)}(\\xi)$$\n",
    "\n",
    "Multiplicando a Equação (2.0) por 4, subtraindo da Equação (2.1), temos:\n",
    "\n",
    "$$3{f}'(x_{0})=\\frac{2}{4h}[f(x_{0}+h) - f(x_{0}-h)]-\\frac{1}{4h}[f(x_{0}+2h) - f(x_{0}-2h)]- \\frac{h^{4}}{30}f^{(5)}(\\tilde{\\xi})+\\frac{2h^{4}}{15}f^{(5)}(\\hat{\\xi})$$\n",
    "\n",
    "Se $f^{(5)}$ for contínua em $[x_{0}-2h,x_{0}+2h]$, podemos usar um método alternativo para mostrar que $f^{(5)}(\\tilde{\\xi})$ e $f^{(5)}(\\hat{\\xi})$ podem ser substituído por um valor comum $f^{(5)}(\\xi)$. Utilizando este resultado e dividindo por 3, obtemos a fórmula de cinco pontos:\n",
    "\n",
    "> $${f}'(x_{0})=\\frac{1}{12h}[f(x_{0}-2h)-8f(x_{0}-h)+8f(x_{0}+h)-f(x_{0}+2h)]+\\frac{h^{4}}{30}f^{(5)}(\\xi)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extrapolação de Richardson"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A extrapolação de Richardson é um __método para aceleração de convergência__, que tem como principal objetivo __aumentar a precisão de outros métodos__ através da aproximação do valor do erro inato de tais métodos. Por este motivo, este procedimento só pode ser utilizado quando for possível obter a fórmula do erro de uma aproximação em função de um parâmetro (geralmente o passo \"h\").\n",
    "\n",
    "Para entender como se obtém um aumento da precisão dos resultados utilizando a extrapolação de Richardson, precisamos relembrar o conceito de __taxa de convergência__, e as convenções anteriormente estabelecidas para a mesma.\n",
    "\n",
    "> A __taxa de convergência__ $O(\\beta_n)$ indica o quão rápido um termo $\\alpha_n$ converge para seu valor real $\\alpha$ de tal forma que $|\\alpha_n-\\alpha| \\leq K|\\beta_n|$, para valores altos de n. Em geral, utilizamos $\\beta_n=\\frac{1}{n^p}$, o que resulta na fórmula $\\alpha_n = \\alpha + O(1/n^p)$.\n",
    "\n",
    "Com a definição acima, podemos perceber que, dada um método qualquer para a aproximação de um valor M da derivada de uma função desconhecida em um ponto $x_0$, podemos escrever uma equação do método que seja do tipo:\n",
    "\n",
    "$$M = N(h) +\\sum_{j=1}^{m-1}K_jh^j + O(h^m)$$\n",
    "\n",
    "Devemos observar que no início deste capítulo assumimos que para as aproximações dos valores das derivadas numéricas em um ponto, utilizaríamos valores pequenos (menores que 1) de h, o que significa que nossa fórmula para a taxa de convergência $O(h^m)$ está de acordo com o padrão desejável estabelecido na definição acima, do tipo $O(1/n^p)$. Além disso, o termo $O(h^m)$ se refere ao erro de truncamento, e pode ser interpretado como \"uma soma de termos de ordem m ou superior\".\n",
    "\n",
    "Observe ainda que, sendo as constantes $K_j$ desconhecidas, e sem grande variação de magnitude, podemos assumir que:\n",
    "\n",
    "$$M-N(0.1) \\approx 0.1K_1, \\quad M-N(0.01) \\approx 0.01 K_1$$\n",
    "\n",
    "e dessa forma, o erro de truncamento seria $O(h)$. Observe que podemos reduzir o valor de $O(h)$, se trabalharmos para obter uma fórmula com $O(h^2)$, que resultaria em:\n",
    "\n",
    "$$M-\\hat{N}(0.1) \\approx 0.001\\hat{K_2}, \\quad M-\\hat{N}(0.01) \\approx 0.0001 \\hat{K_2}$$\n",
    "\n",
    "É dessa maneira que o Método de Richardson funciona, buscando __combinar aproximações dos erros de maneira a obter fórmulas de erro de ordem superior__, o que se traduz em __resultados mais precisos__. Vejamos abaixo como o método funciona.\n",
    "\n",
    "Partindo da equação: $M = N(h) + K_1h + K_2h^2 + K_3h^3+ ...$, cujo erro de truncamento será $O(h)$, podemos substituir $h$ por $h/2$, de maneira a obter:\n",
    "\n",
    "$$M = N(\\frac{h}{2}) + K_1\\frac{h}{2} + K_2\\frac{h^2}{4}+K_3\\frac{h^3}{8}+ ...$$\n",
    "\n",
    "Subtraindo a equação para h da equação para h/2 duplicada, temos:\n",
    "\n",
    "$$M = [2N(\\frac{h}{2}) - N(h)] + K_2(\\frac{h^2}{2} - h^2) + K_3(\\frac{h^3}{4} -h^3) + ...$$\n",
    "\n",
    "Note que na equação acima, o termo acompanhado de $K_1$ foi eliminado pela subtração. Fazendo $N_1(h) \\equiv N(h)$, temos que, o termo entre colchetes na equação anterior é o nosso $N_2(h)$, ou seja:\n",
    "\n",
    "$$N_2(h) = [2N_1(\\frac{h}{2}) - N_1(h)] = N_1(\\frac{h}{2}) + [N_1(\\frac{h}{2}) -N_1(h)]$$\n",
    "\n",
    "Dessa forma, o novo erro de truncamento de nossa equação $M = N_2(h) +  K_2(\\frac{h^2}{2} - h^2) + K_3(\\frac{h^3}{4} -h^3) + ...$ é $O(h^2)$, ou seja. A nova precisão é superior à anterior. \n",
    "\n",
    "Os valores de $N_j$ nada mais são que __versões mais precisas do método utilizado como base para aproximar M__, uma vez que tais aproximações possuem um erro de truncamento $O(h^j)$. Generalizando o procedimento acima, obtemos a fórmula de $N_j$ que é dada por:\n",
    "\n",
    "$$N_j(h) = N_{j-1}(\\frac{h}{2}) + \\frac{N_{j-1}(\\frac{h}{2}) - N_{j-1}(h)}{2^{j-1}-1}$$\n",
    "\n",
    "Que é uma __fórmula de recorrência__, uma vez que utiliza os valores das iterações anteriores para obter valores da nova iteração.\n",
    "\n",
    "O __limite do erro da extrapolação de Richardson__ pode ser estimado calculando-se o valor de K. Para isso, consideramos a equação:\n",
    "\n",
    "$$M = N_j(h) +K_{j}h^j + O(h^{j+1})$$\n",
    "\n",
    "que nada mais é que uma forma alternativa da equação $M = N(h) +\\sum_{j=1}^{m-1}K_jh^j + O(h^m)$, que utiliza o valor dos $N_j$ para deixar somente um coeficiente $K_j$ por vez. Sabemos que, como $h<1$ e o termo $O(h^{j+1})$ é uma função de $h^{j+1}$, este terá um valor menor que $K_jh^j$, e pode ser desconsiderado sem grandes prejuizos (especialmente para valores altos de j). Dessa forma, subtraindo a equação acima com h/2 da mesma equação para h, temos:\n",
    "\n",
    "$$0 = N_j(h) - N_j(\\frac{h}{2}) + K_jh^j-K_j(\\frac{h}{2})^j + O^*(h^{j+1})$$\n",
    "\n",
    "$$0 = N_j(h) - N_j(\\frac{h}{2}) + K_jh^j(1-\\frac{1}{2})^j + O^*(h^{j+1})$$\n",
    "\n",
    "$$ K_j = \\frac{N_j(\\frac{h}{2}) - N_j(h)}{h^j(1-\\frac{1}{2})^j} + O^*(h^{j+1})$$\n",
    "\n",
    "Assim, temos que o erro $E(h)$ pode ser estimado por:\n",
    "\n",
    "$$E(h) = 2^j\\frac{N_j(\\frac{h}{2}) - N_j(h)}{2^j-1} + O(h^{j+1})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Referências:\n",
    "\n",
    "* BURDEN, R.L.;FAIRES,D.J.;BURDEN, A.M. **Numerical Analysis**. 8 ed. Boston, MA: Cengage Learning, 2014, cap. 4, p.167-247. ISBN 978-1-305-25366-7\n",
    "\n",
    "* http://www.mat.ufrgs.br/~fabio/der_int.pdf\n",
    "\n",
    "* http://www.math.ubc.ca/~feldman/m256/richard.pdf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
